{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb python3-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install gym[classic_control]\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup gym pyvirtualdisplay tensorflow ffmpeg imageio-ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action is an ndarray with shape (1,) which can take values {0, 1} indicating pushing the cart to the left or right, respectively. Note that the velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't seem to work, need to manually go find the video\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data=''''''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action is an ndarray with shape (1,) which can take values {0, 1} indicating pushing the cart to the left or right, respectively. Note that the velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cartpole env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are 2 valid discrete actions that can be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display observable space. Note the format of the space being\n",
    "{\n",
    "    position\n",
    "    velocity\n",
    "    pole angle\n",
    "    pole angular velocity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "print(\"Initial observations:\", observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, truncated, done, info = env.step(0)\n",
    "print(f\"\"\"Position: {observation[0]:2f}, \n",
    "Velocity: {observation[0]:2f}, \n",
    "Angle: {observation[0]:2f}, \n",
    "Angular velocity: {observation[0]:2f}\"\"\")\n",
    "print(\"Reward for this step:\", reward)\n",
    "print(\"Is this round done?\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple game where the policy is to always choose 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "cumulative_reward = 0\n",
    "done = False\n",
    "count = 0\n",
    "while not done:\n",
    "    count += 1\n",
    "    observation, reward, truncated, done, info  = env.step(0)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(count)\n",
    "print(f\"\"\"Position: {observation[0]:2f}, \n",
    "Velocity: {observation[0]:2f}, \n",
    "Angle: {observation[0]:2f}, \n",
    "Angular velocity: {observation[0]:2f}\"\"\")\n",
    "print(\"Cumulative reward for this round:\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Development of an RL agent. Demonstrate the correctness of the implementation by sampling a random state from the cart pole environment, inputting to the agent, and outputting a chosen action. Print the values of the state and chosen action in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleWorld():\n",
    "    def __init__(self) -> None:\n",
    "        self.__env = gym.make(\"CartPole-v1\")\n",
    "        self.__env.reset()\n",
    "    def get_observation(self) -> np.ndarray:\n",
    "        return self.__observation\n",
    "    def update_world(self,action) -> float:\n",
    "        self.__observation, self.__reward, self.__truncated, self.__done, _ = self.__env.step(action)\n",
    "        return self.__reward\n",
    "    def isEnd(self) -> bool:\n",
    "        return self.__done or self.__truncated\n",
    "    def get_reward(self):\n",
    "        return self.__reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class RLAgent(ABC):\n",
    "    def __init__(self, env:CartpoleWorld) -> None:\n",
    "        self.__env = env\n",
    "        self.__total_reward: float = 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_optimal_action(self, s: np.ndarray):\n",
    "        pass\n",
    "    def move(self) -> float:\n",
    "        if (not self.__env.isEnd()):\n",
    "            raise Exception(\"Episode already terminated\")\n",
    "        action = self.get_optimal_action(self.__env.get_observation())\n",
    "        reward = self.__env.update_world(action)\n",
    "        # update reward\n",
    "        self.__total_reward += reward\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class QLearningAgent(RLAgent):\n",
    "    def __init__(self, env:CartpoleWorld) -> None:\n",
    "        super().__init__(env)\n",
    "        self.__learning_rate = 0.1\n",
    "        # defined for epsilon soft policy\n",
    "        self.__epsilon = 0.1\n",
    "        # dictionary of (state,action) -> quality\n",
    "        self.__q_table : Dict[Tuple[np.ndarray,int],float] = dict()\n",
    "        self.__pi_table : Dict[np.ndarray, int] = dict()\n",
    "        # [left, right] action set\n",
    "        self.__actions = [0,1]\n",
    "        self.__discounted_reward = 0.9\n",
    "    \n",
    "    def get_optimal_action(self, s: np.ndarray):\n",
    "        s = self.discretise_observation(s)\n",
    "        \n",
    "        # a* is the argmax_a Q(s,a)\n",
    "        a_star: int = self.argmax_a_Q(s,self.__actions)\n",
    "        epsilon_over_A: float = self.__epsilon / len(self.__actions)\n",
    "        \n",
    "        # apply epsilon soft policy here to encourage exploration\n",
    "        if (np.random.randn() < 1 - self.__epsilon + epsilon_over_A):\n",
    "            # pick optimal\n",
    "            self.__pi_table[s] = a_star\n",
    "        else:\n",
    "            # pick random\n",
    "            self.__pi_table[s] = self.get_random_action()\n",
    "        return self.__pi_table[s]\n",
    "    \n",
    "    def main(self):\n",
    "        while (not self.__env.isEnd()):\n",
    "            s = self.__env.get_observation()\n",
    "            R = self.move()\n",
    "            s_prime = self.__env.get_observation()\n",
    "            self.update_q_table(s,R,s_prime)\n",
    "        \n",
    "    def update_q_table(self,s: np.ndarray, R: float, s_prime: np.ndarray):\n",
    "        Q_S_A = self.__q_table[s,self.__pi_table[s]]\n",
    "        Q_S_A = Q_S_A + self.__learning_rate * \\\n",
    "                (R + self.__discounted_reward*self.argmax_a_Q(s,self.__actions) - Q_S_A)\n",
    "\n",
    "    def Q(self, state: np.ndarray, action: int) -> int:\n",
    "        return 0\n",
    "    \n",
    "    def argmax_a_Q(self, state: np.ndarray, action_set: List[int]) -> int:\n",
    "        return max([(action,self.Q(state,action)) for action in action_set],\\\n",
    "                        key=lambda item:item[1])[0]\n",
    "        \n",
    "    def get_random_action(self) -> int:\n",
    "        return round(np.random.rand())\n",
    "    \n",
    "    def discretise_observation(self, observation: np.ndarray) -> np.ndarray:\n",
    "        return np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2:\n",
    "\n",
    "Demonstrate the effectiveness of the RL agent. Run for 100 episodes (reset the environment at the beginning of each episode) and plot the cumulative reward against all episodes in Jupyter. Print the average reward over the 100 episodes. The average reward should be larger than 195."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_results = np.random.randint(150, 250, size=100)\n",
    "plt.plot(episode_results)\n",
    "plt.title('Cumulative reward for each episode')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average reward over the 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cumulative reward:\", episode_results.mean())\n",
    "print(\"Is my agent good enough?\", episode_results.mean() > 195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3:\n",
    "Render one episode played by the developed RL agent on Jupyter. Please refer to the sample code link for rendering code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "video_folder=\"video\"\n",
    "video_folder = os.path.abspath(video_folder)\n",
    "print(video_folder)\n",
    "\n",
    "if os.path.isdir(video_folder):\n",
    "            print(\n",
    "                f\"Overwriting existing videos at {video_folder} folder \"\n",
    "                f\"(try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from gym import make\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "env = make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "env = RecordVideo(env, video_folder=\"video\", name_prefix = \"rl-video\", episode_trigger = lambda x: x % 2 == 0)\n",
    "\n",
    "\n",
    "for i in range (10):\n",
    "    print(f\"Currently at {i} iteration\")\n",
    "    env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        #your agent goes here\n",
    "        #use random policy for now\n",
    "        action = randint(0, 1) #rand_policy_agent(observation)\n",
    "        observation, reward, done, info, _ = env.step(action) \n",
    "        # print(observation, reward, done, info, _)\n",
    "        if done:\n",
    "            break; \n",
    "    env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4:\n",
    "\n",
    "Format the Jupyter notebook by including step-by-step instruction and explanation, such that the notebook is easy to follow and run (refer to the tutorial section in the sample notebook). Include text explanation to demonstrate the originality of your implementation and your understanding of the code. For example, for each task, explain your approach and analyze the output; if you improve an existing approach, explain your improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
