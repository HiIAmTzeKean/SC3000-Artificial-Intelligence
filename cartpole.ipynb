{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb python3-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install gym[classic_control]\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup gym pyvirtualdisplay tensorflow ffmpeg imageio-ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action is an ndarray with shape (1,) which can take values {0, 1} indicating pushing the cart to the left or right, respectively. Note that the velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, List, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "from gym import make\n",
    "import numpy as np\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "import pickle\n",
    "from typing import NamedTuple\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cartpole Environment\n",
    "\n",
    "We define a custom `CartPoleWorld` class to represent the cartpole environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Update CartPoleWord class here\n",
    "'''   \n",
    "class CartpoleWorld():\n",
    "    def __init__(self, display: bool = False) -> None:\n",
    "        if display:\n",
    "            self.__env = make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        else:\n",
    "            self.__env = make(\"CartPole-v1\")\n",
    "        self.__observation: np.ndarray\n",
    "        self.__reward: float = 0\n",
    "        self.__truncated: bool  = False\n",
    "        self.__done: bool = False\n",
    "        self.__observation, _ = self.__env.reset()\n",
    "    \n",
    "    def get_actionspace(self):\n",
    "        return self.__env.action_space\n",
    "        \n",
    "    def get_observation(self) -> np.ndarray:\n",
    "        return self.__observation\n",
    "    \n",
    "    def update_world(self,action) -> float:\n",
    "        self.__observation, self.__reward, self.__truncated, self.__done, _ = self.__env.step(action)\n",
    "        return self.__reward\n",
    "    \n",
    "    def isEnd(self) -> bool:\n",
    "        # position range\n",
    "        if not (-2.4 < self.__observation[0] < 2.4):\n",
    "            return True\n",
    "        # angle range\n",
    "        if not (-.2095 < self.__observation[2] < .2095):\n",
    "            return True\n",
    "        return self.__done or self.__truncated\n",
    "    \n",
    "    def get_reward(self) -> float:\n",
    "        return self.__reward\n",
    "    \n",
    "    def resetWorld(self) -> Observation:\n",
    "        self.__observation, _ = self.__env.reset()\n",
    "        self.__reward = 0\n",
    "        self.__done = False\n",
    "        self.__truncated  = False\n",
    "        return Observation(*self.__observation)\n",
    "        \n",
    "    def set_to_display_mode(self) -> None:\n",
    "        self.__env = make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        self.__observation, _ = self.__env.reset()\n",
    "        \n",
    "    def set_save_video(self) -> None:\n",
    "        self.__env  = make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "        self.__env  = RecordVideo(self.__env , video_folder=\"video\", name_prefix = \"rl-video\")\n",
    "        \n",
    "world = CartpoleWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are 2 valid discrete actions that can be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.get_actionspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display observable space. Note the format of the space being\n",
    "{\n",
    "    position,\n",
    "    velocity,\n",
    "    pole angle,\n",
    "    pole angular velocity,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.resetWorld()\n",
    "print(\"Initial observations:\", world.get_observation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Development of an RL agent. Demonstrate the correctness of the implementation by sampling a random state from the cart pole environment, inputting to the agent, and outputting a chosen action. Print the values of the state and chosen action in Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "We define an `Observation` class as a `NamedTuple`, allowing us to retrive the value by index and by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observation(NamedTuple):\n",
    "    d: float\n",
    "    x: float\n",
    "    theta: float\n",
    "    omega: float\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base RL Agent\n",
    "\n",
    "We define an abstract base RL agent as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(ABC):\n",
    "    def __init__(self, env:CartpoleWorld) -> None:\n",
    "        self._env = env\n",
    "        self._total_reward: float = 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_optimal_action(self, s: Observation) -> int:\n",
    "        pass\n",
    "    \n",
    "    def move(self, state: Observation) -> float:\n",
    "        if (self._env.isEnd()):\n",
    "            raise Exception(\"Episode already terminated\")\n",
    "        action = self.get_optimal_action(state)\n",
    "        reward = self._env.update_world(action)\n",
    "        # update reward\n",
    "        self._total_reward += reward\n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    def run_training(self, num_of_episode: int) -> None:\n",
    "        pass\n",
    "    \n",
    "    def run_single_episode_training(self) -> int:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_production(self, num_of_episode: int) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run_single_episode_production(self) -> int:\n",
    "        pass\n",
    "    \n",
    "    def wrap_observation(self, observation: np.ndarray) -> Observation:\n",
    "        \"\"\"Converts numpy array to Observation object\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): array to pass in from cartpole\n",
    "\n",
    "        Returns:\n",
    "            Observation: Object to return\n",
    "        \"\"\"\n",
    "        return Observation(*observation)\n",
    "    \n",
    "    def discretise_observation(self, observation: np.ndarray) -> Observation:\n",
    "        # Position round off to 0.1 precision\n",
    "        # Velocity round off to whole\n",
    "        # Angle round off to 0.01 precision\n",
    "        # Velocity round off to whole\n",
    "        observation[0] = round(observation[0],1)\n",
    "        observation[1] = round(observation[1],0)\n",
    "        observation[2] = round(observation[2],2)\n",
    "        observation[3] = round(observation[3],0)\n",
    "        return Observation(*observation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Monte Carlo Agent\n",
    "\n",
    "We first attempt to implement a Monte Carlo agent. Being the very first topic introduced in the lecture of Reinforcement Learning, it is also one of the easiest to implement. We use this agent as a baseline performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MC Agent Here\n",
    "'''\n",
    "class MCAgent():\n",
    "    def __init__(self, env: CartpoleWorld) -> None:\n",
    "        self.env = env\n",
    "        self.total_reward = 0\n",
    "        self.actions = [0,1]\n",
    "        self.exploit_rate = 0\n",
    "        self.gamma = 0.9\n",
    "        self.actual = False\n",
    "\n",
    "        # Format: dict[(state,action)] = [count, score]\n",
    "        self.Q = defaultdict(lambda: [0, 0])\n",
    "\n",
    "    def discretise_observation(self, observation: np.ndarray) -> Observation:\n",
    "        # Position round off to 0.1 precision\n",
    "        # Velocity round off to whole\n",
    "        # Angle round off to 0.01 precision\n",
    "        # Velocity round off to whole\n",
    "        observation[0] = round(observation[0], 1)\n",
    "        observation[1] = round(observation[1], 0)\n",
    "        observation[2] = round(observation[2], 2)\n",
    "        observation[3] = round(observation[3], 0)\n",
    "        return Observation(*observation)\n",
    "    \n",
    "    def geometric_progression(self, n: int) -> float:\n",
    "        # Calcuate rewards based on the formula S = a(1 - r^n) / 1 - r\n",
    "        return (1 - (self.gamma ** n)) / (1 - self.gamma)\n",
    "\n",
    "\n",
    "    def run_single_episode(self) -> int:\n",
    "        # clear history\n",
    "        self.env.resetWorld()\n",
    "        self.total_reward = 0\n",
    "\n",
    "        history = []\n",
    "\n",
    "        while (not self.env.isEnd()):\n",
    "            ndarry: np.ndarray = self.env.get_observation()\n",
    "            s = Observation(*self.discretise_observation(ndarry))\n",
    "\n",
    "            action = self.get_optimal_action(s)\n",
    "            reward = self.env.update_world(action)\n",
    "            history.append((s, action))\n",
    "            # update reward\n",
    "            self.total_reward += reward\n",
    "\n",
    "        self.update_q_table(history)\n",
    "        #self.max_reward = max(self.max_reward, self.total_reward)\n",
    "        return self.total_reward\n",
    "    \n",
    "\n",
    "    def update_q_table(self, history: List[Tuple[Observation, int]]) -> None:\n",
    "        state_action_pair = defaultdict(int)\n",
    "        # Adopts first visit by updating from the back\n",
    "        # Hence, only the first occurence of each (state,action) pair will be recorded\n",
    "        # n is the episode length starting from the back, which is also the reward \n",
    "        for n,val in enumerate(history[::-1], start=1):\n",
    "            state_action_pair[val] = n\n",
    "\n",
    "        for key,val in state_action_pair.items():\n",
    "            count, score = self.Q[key]\n",
    "            score = (count * score + val) / (count + 1)\n",
    "            self.Q[key] = [count+1, score]\n",
    "\n",
    "\n",
    "    def get_optimal_action(self, s: Observation):\n",
    "        \n",
    "        #Slowly increase the exploitation rate, such that the agent will choose the best action as time progresses\n",
    "        if not self.actual and random.random() >= self.exploit_rate:\n",
    "            #return random.randint(0, 1)\n",
    "            \n",
    "            action = 1 if s[2] > 0 else 0\n",
    "            # Chooses the physics action which a probability of 0.6, and the other action with a prob of 0.4\n",
    "            return action if random.random() <= 0.60 else 1 - action\n",
    "        \n",
    "        return max(self.actions, key=lambda a: self.Q[(s,a)])\n",
    "    \n",
    "\n",
    "    def run(self, num_episodes: int, display: bool = False) -> None:\n",
    "        cumulated_reward = 0\n",
    "\n",
    "        # Pre-training, maybe can replace with pre-load data here in the future\n",
    "        # Run 1000 times first using the physics solution to populate values\n",
    "        self.exploit_rate = 0.0\n",
    "        for _ in range(1000):    \n",
    "            self.run_single_episode()\n",
    "\n",
    "\n",
    "        outer_range, inner_range = 10, 5000\n",
    "        self.exploit_rate = 0.7\n",
    "\n",
    "        for _ in range(outer_range):\n",
    "            total = 0\n",
    "            max_reward = 0\n",
    "            for _ in range(inner_range):\n",
    "                # Using a variable exploitation rate, dk if its a good idea\n",
    "                #self.exploit_rate = i / inner_range\n",
    "                reward = self.run_single_episode()\n",
    "                total += reward\n",
    "                max_reward = max(max_reward, reward)\n",
    "                \n",
    "\n",
    "            print(f\"Mean reward is: {total / inner_range}\")\n",
    "            print(f\"Max reward is: {max_reward}\")\n",
    "\n",
    "        # Actual run\n",
    "        self.actual = True\n",
    "\n",
    "        if display:\n",
    "            self.env.set_to_display_mode()\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            current_reward = self.run_single_episode()\n",
    "            cumulated_reward += current_reward\n",
    "\n",
    "mc_agent = MCAgent(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Monte Carlo Agent\n",
    "\n",
    "### <span style=\"color:red\">ToDo:</span>\n",
    " \n",
    "  - Find stuff talking about MC sampling\n",
    "  - Motivation for using a hybrid initial policy (as compared to pure random sampling)\n",
    "  - Motivation for using a decaying epsilon rate\n",
    "  - How does the agent obtain the optimal action\n",
    "  - How does the agent update the q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctness of implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = world.resetWorld()\n",
    "action = mc_agent.get_optimal_action(observation)\n",
    "print(f\"{observation}\")\n",
    "print(f\"Chosen Action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Q-Learning Agent\n",
    "\n",
    "The second agent we implemented was a Q-Learning Agent. Being the second topic taught in the lecture, it is an off-policy reinforcement learning that will find the best course of action, given the current state of the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q-learning Agent here\n",
    "'''\n",
    "class QLearningAgent(RLAgent):\n",
    "    def __init__(self, env:CartpoleWorld) -> None:\n",
    "        super().__init__(env)\n",
    "        \n",
    "        # defines learning rate\n",
    "        self.__learning_rate = 0.5\n",
    "        self.__learning_rate_min = 0.2\n",
    "        \n",
    "        # defined for epsilon soft policy\n",
    "        # initially set to a large number to encourage exploration\n",
    "        # epsilon will decay as episodes increase\n",
    "        self.__epsilon = 0.9\n",
    "        self._epsilon_min = 0.1\n",
    "        \n",
    "        # dictionary of (state,action) -> quality\n",
    "        self.__q_table : Dict[Tuple[Observation,int],float] = dict()\n",
    "        self.__pi_table : Dict[Observation, int] = dict()\n",
    "        \n",
    "        # Load pickle file to restore agent previous state\n",
    "        self.load_pickle('QL_parameters.pkl')\n",
    "        \n",
    "        # [left, right] action set\n",
    "        self.__actions = [0,1]\n",
    "        self.__discounted_reward = 0.9\n",
    "        \n",
    "        # parameter for production\n",
    "        self.__is_production = False\n",
    "        \n",
    "    \n",
    "    def get_optimal_action(self, s: Observation):\n",
    "        # a* is the argmax_a Q(s,a)\n",
    "        a_star: int = self.argmax_a_Q(s,self.__actions)\n",
    "        \n",
    "        if (self.__is_production):\n",
    "            return a_star\n",
    "        \n",
    "        epsilon_over_A: float = self.__epsilon / len(self.__actions)\n",
    "        \n",
    "        # apply epsilon soft policy here to encourage exploration\n",
    "        if (np.random.randn() < 1 - self.__epsilon + epsilon_over_A):\n",
    "            # pick optimal\n",
    "            self.__pi_table[s] = a_star\n",
    "        else:\n",
    "            # pick random\n",
    "            self.__pi_table[s] = self.get_random_action()\n",
    "        return self.__pi_table[s]\n",
    "    \n",
    "    def update_parameters(self) -> None:\n",
    "        self.decay_epsilon()\n",
    "        self.decay_learning_rate()\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        if self.__epsilon <= self._epsilon_min:\n",
    "            return\n",
    "        self.__epsilon *= 0.999999\n",
    "    \n",
    "    def decay_learning_rate(self) -> None:\n",
    "        if self.__learning_rate <= self.__learning_rate_min:\n",
    "            return\n",
    "        self.__learning_rate *= 0.9999999\n",
    "    \n",
    "    def run_training(self,  num_of_episode: int):\n",
    "        \"\"\"Overrides base class method\n",
    "\n",
    "        Args:\n",
    "            num_of_episode (int): Number of episdoe to run\n",
    "        \"\"\"\n",
    "        cumulated_reward = 0\n",
    "        for _ in range(num_of_episode):\n",
    "            cumulated_reward += self.run_single_episode()\n",
    "        \n",
    "        print(f\"Epsilon: {self.__epsilon}, Discounted reward: {self.__discounted_reward}, Learning rate: {self.__learning_rate}\")\n",
    "        print(f\"Mean reward is: {cumulated_reward/num_of_episode} for {num_of_episode} episodes\")\n",
    "        self.save_pickle(\"QL_parameters.pkl\")\n",
    "    \n",
    "    def run_production(self, num_of_episode: int):\n",
    "        self.__is_production = True\n",
    "        \n",
    "        cumulated_reward = 0\n",
    "        for _ in range(num_of_episode):\n",
    "            cumulated_reward += self.run_single_episode_production()\n",
    "\n",
    "        print(f\"Mean reward is: {cumulated_reward/num_of_episode} for {num_of_episode} episodes\")\n",
    "\n",
    "    def run_single_episode_training(self) -> int:\n",
    "        # clear history\n",
    "        self._env.resetWorld()\n",
    "        self._total_reward = 0\n",
    "        \n",
    "        s_prime = self._env.get_observation()\n",
    "        s_prime = self.discretise_observation(s_prime)\n",
    "        \n",
    "        while (not self._env.isEnd()):\n",
    "            s = s_prime\n",
    "            R = self.move(s)\n",
    "            s_prime = self._env.get_observation()\n",
    "            s_prime = self.discretise_observation(s_prime)\n",
    "            \n",
    "            self.update_q_table(s,R,s_prime)\n",
    "            self.update_parameters()\n",
    "        return self._total_reward\n",
    "    \n",
    "    def run_single_episode_production(self) -> int:\n",
    "        # clear history\n",
    "        self._env.resetWorld()\n",
    "        self._total_reward = 0\n",
    "        \n",
    "        s_prime = self._env.get_observation()\n",
    "        s_prime = self.discretise_observation(s_prime)\n",
    "        \n",
    "        while (not self._env.isEnd()):\n",
    "            s = s_prime\n",
    "            R = self.move(s)\n",
    "            s_prime = self._env.get_observation()\n",
    "            s_prime = self.discretise_observation(s_prime)\n",
    "        return self._total_reward\n",
    "\n",
    "    def update_q_table(self,s: Observation, R: float, s_prime: Observation):\n",
    "        Q_S_A = self.__q_table[(s,self.__pi_table[s])]\n",
    "        Q_S_A = Q_S_A + self.__learning_rate * \\\n",
    "                (R + self.__discounted_reward*self.Q(s_prime, self.argmax_a_Q(s_prime,self.__actions)) - Q_S_A)\n",
    "        \n",
    "        self.__q_table[(s,self.__pi_table[s])] = Q_S_A\n",
    "\n",
    "    def Q(self, state: Observation, action: int) -> float:\n",
    "        if ((state,action) in self.__q_table):\n",
    "            return self.__q_table[(state,action)]\n",
    "        else:\n",
    "            self.__q_table[(state,action)] = 0\n",
    "            return 0\n",
    "    \n",
    "    def argmax_a_Q(self, state: Observation, action_set: List[int]) -> int:\n",
    "        \"\"\"Returns action that maximises Q function\n",
    "\n",
    "        Args:\n",
    "            state (Observation): state observed\n",
    "            action_set (List[int]): list of actions possible\n",
    "\n",
    "        Returns:\n",
    "            int: action\n",
    "        \"\"\"\n",
    "        return max([(action,self.Q(state,action)) for action in action_set],key=lambda item:item[1])[0]\n",
    "       \n",
    "    # todo: action set should be passed into the function for consistency \n",
    "    def get_random_action(self) -> int:\n",
    "        \"\"\"Randomly generates an action.\n",
    "        \n",
    "        Returns:\n",
    "            int: Action taken\n",
    "        \"\"\"\n",
    "        val = np.random.rand()\n",
    "        if (float(val) % 1) >= 0.5:\n",
    "            return math.ceil(val)\n",
    "        else:\n",
    "            return round(val)\n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(self.__q_table)\n",
    "        \n",
    "    def get_q_table(self):\n",
    "        return self.__q_table\n",
    "    \n",
    "    def get_pi_table(self):\n",
    "        return self.__pi_table\n",
    "        \n",
    "    def load_pickle(self, parameters_file: str):\n",
    "        \"\"\"Loads pickle file to agent table\n",
    "\n",
    "        Args:\n",
    "            parameters (str): pickle file location\n",
    "        \"\"\"\n",
    "        if os.path.exists(parameters_file):\n",
    "            with open(parameters_file, 'rb') as file:\n",
    "                # Call load method to deserialze\n",
    "                self.__pi_table,self.__q_table,self.__epsilon,self.__learning_rate = pickle.load(file)\n",
    "        else:\n",
    "            print(\"*** LOG: Pickle file not found\")\n",
    "            pass\n",
    "        \n",
    "    def save_pickle(self, parameters_file: str):\n",
    "        \"\"\"Saves q and pi table to pickle.\n",
    "\n",
    "        Args:\n",
    "            pi_table_file (str): location of file\n",
    "            q_table_file (str): location of file\n",
    "        \"\"\"\n",
    "        with open(parameters_file, 'wb') as file:\n",
    "            # Call load method to deserialze\n",
    "            pickle.dump([self.__pi_table,self.__q_table,self.__epsilon,self.__learning_rate], file)\n",
    "\n",
    "qlearning_agent = QLearningAgent(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Q-Learning Agent\n",
    "\n",
    "### <span style=\"color:red\">ToDo:</span>\n",
    " \n",
    "  - Find stuff talking about temporal difference\n",
    "  - Motivation for using a decaying epsilon rate\n",
    "  - How does the agent obtain the optimal action\n",
    "  - How does the agent update the q-table and pi-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctness of implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = world.resetWorld()\n",
    "action = qlearning_agent.get_optimal_action(observation)\n",
    "print(f\"{observation}\")\n",
    "print(f\"Chosen Action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2:\n",
    "\n",
    "Demonstrate the effectiveness of the RL agent. Run for 100 episodes (reset the environment at the beginning of each episode) and plot the cumulative reward against all episodes in Jupyter. Print the average reward over the 100 episodes. The average reward should be larger than 195."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Effectiveness of MC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_results = np.asarray([mc_agent.run_single_episode() for _ in range(100)])\n",
    "\n",
    "plt.plot(episode_results)\n",
    "plt.title('Cumulative reward for each episode')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average reward over the 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Average cumulative reward:\", episode_results.mean())\n",
    "print(\"Is my agent good enough?\", episode_results.mean() > 195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Effectiveness of Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_results = np.asarray([qlearning_agent.run_single_episode_production() for _ in range(100)])\n",
    "\n",
    "plt.plot(episode_results)\n",
    "plt.title('Cumulative reward for each episode')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Average cumulative reward:\", episode_results.mean())\n",
    "print(\"Is my agent good enough?\", episode_results.mean() > 195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "### <span style=\"color:red\">ToDo: Write observations</span>.\n",
    "\n",
    "- MC Agent suck balls\n",
    "- Q-Learning good\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "### <span style=\"color:red\">ToDo: Write conclusions</span>.\n",
    "\n",
    "- Use Q-Learning \n",
    "\n",
    "\n",
    "Attach reference for proof of q-learning converging to the optimum policy.\n",
    "\n",
    "Hence, by running a sizably large number of episodes, we would expect the q-learning agent to converge to the optimum policy (Appendix A), thus achieving the maximum rewards of 500. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3:\n",
    "Render one episode played by the developed RL agent on Jupyter. Please refer to the sample code link for rendering code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run one episode of the q-learning agent here, with render = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4:\n",
    "\n",
    "Format the Jupyter notebook by including step-by-step instruction and explanation, such that the notebook is easy to follow and run (refer to the tutorial section in the sample notebook). Include text explanation to demonstrate the originality of your implementation and your understanding of the code. For example, for each task, explain your approach and analyze the output; if you improve an existing approach, explain your improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Optimal Solution using Physics\n",
    "\n",
    "[Tang, Eric. \"How to Beat the Cartpole Game in 5 Lines.\" Towards Data Science, 23 Apr. 2018](https://towardsdatascience.com/how-to-beat-the-cartpole-game-in-5-lines-5ab4e738c93f)\n",
    "\n",
    "In our research, we came across this article which analyzes the cartpole problem. By identifying the physics behind the problem, they were able to derive the optimal policy as follows:\n",
    "\n",
    "- When the angle θ is “small”, we want to stabilize θ. This is the same as the Omega Policy.\n",
    "- When the angle θ is “large”, we want to correct θ, i.e., give an angular acceleration towards the center.\n",
    "\n",
    "We implement a physics agent using the optimal policy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsAgent(RLAgent):\n",
    "    \n",
    "    def __init__(self, env:CartpoleWorld) -> None:\n",
    "        super().__init__(env)\n",
    "\n",
    "    def run_production(self, num_of_episode: int) -> None:\n",
    "        cumulated_reward = 0\n",
    "        for _ in range(num_of_episode):\n",
    "            current_reward = self.run_single_episode_production()\n",
    "            cumulated_reward += current_reward\n",
    "        print(f\"Mean reward is: {cumulated_reward/num_of_episode}\")\n",
    "\n",
    "    def run_single_episode_production(self) -> int:\n",
    "        # clear history\n",
    "        self._env.resetWorld()\n",
    "        self._total_reward = 0\n",
    "        while (not self._env.isEnd()):\n",
    "            ndarry: Observation = self._env.get_observation()\n",
    "            s = self.wrap_observation(ndarry)\n",
    "            self.move(s)\n",
    "        return self._total_reward\n",
    "    \n",
    "    \n",
    "    def get_optimal_action(self, s: Observation) -> int:\n",
    "        \"\"\"Reference: https://towardsdatascience.com/how-to-beat-the-cartpole-game-in-5-lines-5ab4e738c93f\n",
    "            Overrides abstract base class\n",
    "        \"\"\"\n",
    "        theta, w = s[2:4]\n",
    "        if abs(theta) < 0.03:\n",
    "            return 0 if w < 0 else 1\n",
    "        else:\n",
    "            return 0 if theta < 0 else 1\n",
    "        \n",
    "physics_agent = PhysicsAgent(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectiveness of optimal Physics Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_results = np.asarray([physics_agent.run_single_episode_production() for _ in range(100)])\n",
    "\n",
    "plt.plot(episode_results)\n",
    "plt.title('Cumulative reward for each episode')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render one episode played by the optimal physics agent on Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run one episode of the physics agent here, with render = True\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
